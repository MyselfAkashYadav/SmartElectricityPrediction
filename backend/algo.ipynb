{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this in goole collab \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install necessary dependencies\n",
    "!pip install flask flask-cors pyngrok scikit-learn\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "!dpkg -i cloudflared-linux-amd64.deb\n",
    "\n",
    "# Step 2: Import necessary libraries and initialize Flask, Spark, and AdaBoost\n",
    "from flask import Flask, jsonify\n",
    "from flask_cors import CORS\n",
    "import requests\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, dayofyear\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ElectricityData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# API configuration\n",
    "API_URL = \"https://api.eia.gov/v2/electricity/rto/daily-region-data/data/\"\n",
    "API_KEY = \"8pT7GUYm3TYXQqAbGvm65saYJqwzlYHeIEwcdC53\"\n",
    "\n",
    "# Function to get data from the API and load into Spark DataFrame\n",
    "def get_api_data():\n",
    "    params = {\n",
    "        \"frequency\": \"daily\",\n",
    "        \"data[0]\": \"value\",\n",
    "        \"sort[0][column]\": \"period\",\n",
    "        \"sort[0][direction]\": \"desc\",\n",
    "        \"offset\": \"0\",\n",
    "        \"length\": \"5000\",\n",
    "        \"api_key\": API_KEY,\n",
    "    }\n",
    "    response = requests.get(API_URL, params=params)\n",
    "    data = response.json()\n",
    "    raw_df = spark.createDataFrame(data[\"response\"][\"data\"])\n",
    "\n",
    "    # Convert the 'value' column to float\n",
    "    raw_df = raw_df.withColumn(\"value\", col(\"value\").cast(\"float\"))\n",
    "    return raw_df\n",
    "\n",
    "# Function to perform K-means clustering using PySpark\n",
    "def perform_kmeans(df, n_clusters=3):\n",
    "    assembler = VectorAssembler(inputCols=[\"value\"], outputCol=\"features\")\n",
    "    assembled_df = assembler.transform(df)\n",
    "\n",
    "    kmeans = KMeans().setK(n_clusters).setSeed(42).setFeaturesCol(\"features\").setPredictionCol(\"cluster\")\n",
    "    model = kmeans.fit(assembled_df)\n",
    "    clustered_df = model.transform(assembled_df)\n",
    "    return clustered_df\n",
    "\n",
    "# Function to generate the cluster plot for K-means and AdaBoost\n",
    "def generate_plot(clustered_df, adaboost_predictions):\n",
    "    pandas_df = clustered_df.select(\"cluster\", \"value\").toPandas()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plot K-means clusters\n",
    "    for cluster in pandas_df['cluster'].unique():\n",
    "        cluster_data = pandas_df[pandas_df['cluster'] == cluster]\n",
    "        plt.scatter(cluster_data.index, cluster_data['value'], label=f\"K-means Cluster {cluster}\", alpha=0.6)\n",
    "\n",
    "    # Plot AdaBoost predictions\n",
    "    plt.plot(adaboost_predictions.index, adaboost_predictions.values, label=\"AdaBoost Regression\", color=\"purple\", linewidth=2)\n",
    "\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value (MWh)')\n",
    "    plt.title('Clustering and Regression of Electricity Data')\n",
    "    plt.legend()\n",
    "\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plot_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    buf.close()\n",
    "    return plot_data\n",
    "\n",
    "# Function to predict the weekly electricity bill using both Linear Regression and AdaBoost\n",
    "def predict_weekly_bill(df):\n",
    "    # Convert 'period' to numeric day of the year\n",
    "    df = df.withColumn(\"day\", dayofyear(to_date(col(\"period\"))))\n",
    "\n",
    "    # Prepare data for Linear Regression\n",
    "    assembler = VectorAssembler(inputCols=[\"day\"], outputCol=\"features\")\n",
    "    data = assembler.transform(df.select(\"day\", col(\"value\").cast(\"float\").alias(\"label\")))\n",
    "\n",
    "    # Train Linear Regression model\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    lr_model = lr.fit(data)\n",
    "\n",
    "    # Predict usage for the next 7 days using Linear Regression\n",
    "    max_day = df.agg({\"day\": \"max\"}).collect()[0][0]\n",
    "    future_days = spark.createDataFrame([(max_day + i,) for i in range(1, 8)], [\"day\"])\n",
    "    future_data = assembler.transform(future_days)\n",
    "    lr_predictions = lr_model.transform(future_data).select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Prepare data for AdaBoost\n",
    "    pandas_df = df.select(\"day\", \"value\").toPandas()\n",
    "    adaboost_model = AdaBoostRegressor()\n",
    "    adaboost_model.fit(pandas_df[['day']], pandas_df['value'])\n",
    "    adaboost_predictions = adaboost_model.predict([[max_day + i] for i in range(1, 8)])\n",
    "\n",
    "    # Assume a cost of $0.12 per unit (kWh) for weekly prediction\n",
    "    rate_per_unit = 0.12\n",
    "    weekly_bill_lr = round(sum(lr_predictions) * rate_per_unit, 2)\n",
    "    weekly_bill_adaboost = round(sum(adaboost_predictions) * rate_per_unit, 2)\n",
    "\n",
    "    return weekly_bill_lr, weekly_bill_adaboost, pd.Series(adaboost_predictions)\n",
    "\n",
    "@app.route('/cluster', methods=['GET'])\n",
    "def cluster_data():\n",
    "    try:\n",
    "        # Fetch the data and perform clustering\n",
    "        df = get_api_data()\n",
    "        clustered_df = perform_kmeans(df)\n",
    "\n",
    "        # Predict the weekly bill and generate comparative plot\n",
    "        weekly_bill_lr, weekly_bill_adaboost, adaboost_predictions = predict_weekly_bill(df)\n",
    "        plot_data = generate_plot(clustered_df, adaboost_predictions)\n",
    "\n",
    "        response = {\n",
    "            \"data\": clustered_df.select(\"period\", \"value\", \"cluster\").toPandas().to_dict(orient='records'),\n",
    "            \"plot\": f\"data:image/png;base64,{plot_data}\",\n",
    "            \"weekly_bill_lr\": weekly_bill_lr,\n",
    "            \"weekly_bill_adaboost\": weekly_bill_adaboost\n",
    "        }\n",
    "        return jsonify(response)\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Step 3: Run the Flask app in the background\n",
    "import threading\n",
    "\n",
    "def run_app():\n",
    "    app.run(host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "thread = threading.Thread(target=run_app)\n",
    "thread.start()\n",
    "\n",
    "# Step 4: Start cloudflared to expose the Flask app publicly\n",
    "!cloudflared tunnel --url http://localhost:8000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
